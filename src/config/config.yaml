model:
  in_channels: 3
  out_channels: 3
  sfeb_channels: 48 # Width of the network (W)
  num_blocks: 8 # Number of adaptive MaxViT blocks (B)
  num_heads: 8 # Number of heads for multi-head attention
  ffn_dim: 192 # Feed-forward network dimension (typically 4x the width)
  upscale_factor: 2 # Upscale factor for the final output

sfeb:
  out_channels: 64

adaptive_grid_attention:
  grid_size: 8
  hidden_dim: 64
  dropout: 0.1

hffb:
  in_channels: 64

rb:
  in_channels: 64
  out_channels: 3
  upscale_factor: 2
