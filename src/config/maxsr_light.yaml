model_config:
  patch_size: 64
  num_patches: 64
  output_size: 256
  channels: 3
  emb_size: 768
  num_heads: 8
  depth: 6
  scale_factor: 4 # This represents the upscaling factor for spatial dimensions
  block_size: 8 # Example block size for attention mechanisms
  dim: 768 # Dimension for transformer embeddings, matching SFEB output
  num_features: 4
  stages: 4
  block_per_stage: 2
  final_image_size: 2048
  patch_target_size: 256 # after reconstruction each patch is (patch size * scale factor)
  hidden_features: 256
